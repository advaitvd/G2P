{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d6a2310a-641e-498f-a661-a8fc53d90ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8b871ef8-7292-4e07-a998-c110d9f3e807",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Users/advait.d/Documents/G2P/data/cmudict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8a002baf-5015-48db-a5cc-a6714e6a42f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmudict_dict = 'cmudict.dict'\n",
    "cmudict_symbols = 'cmudict.symbols'\n",
    "cmudict_phones = 'cmudict.phones'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2c087060-79e2-4f36-a1c9-49f17e8404d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_path = os.path.join(data_path, cmudict_dict)\n",
    "symbols_path = os.path.join(data_path, cmudict_symbols)\n",
    "# phones_path = os.path.join(data_path, cmudict_phones)\n",
    "\n",
    "idx2chr = \"a b c d e f g h i j k l m n o p q r s t u v w x y z ' <pad>\".split(' ')\n",
    "chr2idx = {ch: idx for idx, ch in enumerate(idx2chr)}\n",
    "\n",
    "def parse_phones_file():\n",
    "    with open(symbols_path, 'r') as f:\n",
    "        lines = f.read().strip().split('\\n')\n",
    "\n",
    "    ph2idx = {phone: idx for idx, phone in enumerate(lines)}\n",
    "    sos_token = \"<sos>\"\n",
    "    eos_token = \"<eos>\"\n",
    "    pad_token = \"<pad>\"\n",
    "    ph2idx[sos_token] = len(ph2idx)\n",
    "    ph2idx[eos_token] = len(ph2idx)\n",
    "    ph2idx[pad_token] = len(ph2idx)\n",
    "    idx2ph = {idx: ph for ph, idx in ph2idx.items()}\n",
    "    return ph2idx, idx2ph, sos_token, eos_token, pad_token\n",
    "\n",
    "ph2idx, idx2ph, sos_token, eos_token, pad_token = parse_phones_file()\n",
    "\n",
    "def tokenize(words, phoness):\n",
    "    words_tokens = []\n",
    "    phones_tokens = []\n",
    "    for word, phones in zip(words, phoness):\n",
    "        words_tokens.append([chr2idx[c] for c in word if c in chr2idx])\n",
    "        phones_tokens.append([ph2idx[ph] for ph in phones])\n",
    "\n",
    "    return words_tokens, phones_tokens\n",
    "\n",
    "def parse_cmudict():\n",
    "    with open(dict_path, 'r') as f:\n",
    "        data = f.read().strip().split('\\n')\n",
    "\n",
    "    words, phones = [], []\n",
    "    for point in data:\n",
    "        point = point.split('#')[0].strip()\n",
    "        word, ph = point.split(' ')[0], point.split(' ')[1:]\n",
    "        ph = [sos_token] + ph + [eos_token]\n",
    "        word = [c for c in word]\n",
    "        \n",
    "        words.append(word)\n",
    "        phones.append(ph)\n",
    "\n",
    "    return words, phones\n",
    "\n",
    "words, phones = parse_cmudict()\n",
    "word_tokens, phone_tokens = tokenize(words, phones)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c0d1a954-437d-4d58-a366-7cab256e20c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = len(ph2idx)\n",
    "input_size = len(chr2idx)\n",
    "hidden_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fb9eefac-d939-456b-b7b3-8094b1e110aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ff377307-589b-4096-91da-5c9f74ecabb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[26, 1, 14, 20, 19],\n",
       " [26, 2, 0, 20, 18, 4],\n",
       " [26, 2, 14, 20, 17, 18, 4],\n",
       " [26, 2, 20, 18, 4],\n",
       " [26, 4, 12]]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3cb32cf2-992c-4bf0-9954-af81c238c730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[84, 24, 18, 69, 85],\n",
       " [84, 52, 9, 82, 85],\n",
       " [84, 52, 14, 66, 67, 85],\n",
       " [84, 52, 81, 77, 82, 85],\n",
       " [84, 9, 54, 85]]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone_tokens[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "203da942-f205-446d-be2b-04d73cc7dd86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(135167, 135167)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_tokens), len(phone_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "baeb222b-a027-4948-b4ee-60461fcb273a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import heapq\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cec8e1f",
   "metadata": {},
   "source": [
    "# BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e2144fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, bidirectional=True, batch_first=True)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        embedded = self.embedding(input_seq)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_step, hidden, cell):\n",
    "        embedded = self.embedding(input_step)\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        prediction = self.fc(output)\n",
    "        return prediction, hidden, cell\n",
    "    \n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5, device='cpu'):\n",
    "        batch_size = src.shape[0]  # Adjusted for batch_first=True\n",
    "        max_len = trg.shape[1]     # Adjusted for batch_first=True\n",
    "        trg_vocab_size = self.decoder.output_size\n",
    "\n",
    "        outputs = torch.zeros(batch_size, max_len, trg_vocab_size).to(device)\n",
    "\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "\n",
    "        hidden_forward = hidden[0::2,:,:]\n",
    "        hidden_backward = hidden[1::2,:,:]\n",
    "        hidden = hidden_forward + hidden_backward\n",
    "\n",
    "        cell = torch.zeros(self.decoder.num_layers, batch_size, self.decoder.hidden_size).to(device)\n",
    "\n",
    "        input_step = trg[:, 0:1]  # Adjusted for batch_first=True\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden, cell = self.decoder(input_step, hidden, cell)\n",
    "            outputs[:, t:t+1, :] = output  # Adjusted for batch_first=True\n",
    "            top1 = output.argmax(-1)\n",
    "            input_step = trg[:, t:t+1] if random.random() < teacher_forcing_ratio else top1\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "\n",
    "def beam_search(model, src, max_len, beam_width=3, device='cpu'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden = model.encoder(src)\n",
    "\n",
    "        hidden_forward = hidden[0::2,:,:]\n",
    "        hidden_backward = hidden[1::2,:,:]\n",
    "        hidden = hidden_forward + hidden_backward\n",
    "\n",
    "        cell = torch.zeros(model.decoder.num_layers, 1, model.decoder.hidden_size).to(device)\n",
    "\n",
    "\n",
    "        input_step = torch.tensor([ph2idx[sos_token]], device=device)\n",
    "\n",
    "        beam = [(0, torch.tensor([input_step]), hidden, cell)]  # (score, sequence, hidden, cell)\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            new_beam = []\n",
    "            for score, seq, hidden, cell in beam:\n",
    "                output, hidden, cell = model.decoder(seq[-1:].unsqueeze(0), hidden, cell)\n",
    "                log_probs = F.log_softmax(output, dim=-1)\n",
    "                topk_log_probs, topk_indices = log_probs.topk(beam_width)\n",
    "\n",
    "                for i in range(beam_width):\n",
    "                    new_score = score + topk_log_probs[0][0][i].item()\n",
    "                    new_seq = torch.concat([seq, torch.tensor([topk_indices[0][0][i]])])\n",
    "                    new_beam.append((new_score, new_seq, hidden, cell))\n",
    "\n",
    "            beam = heapq.nlargest(beam_width, new_beam, key=lambda x: x[0])\n",
    "\n",
    "            if any(seq[-1].item() == ph2idx[eos_token] for _, seq, _, _ in beam):\n",
    "                break\n",
    "\n",
    "        best_score, best_seq, _, _ = max(beam, key=lambda x: x[0])\n",
    "        return [token.item() for token in best_seq if token.item() != ph2idx[sos_token]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "442bcb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    word_batch, phone_batch = zip(*batch)\n",
    "    word_batch = pad_sequence(word_batch, batch_first=True, padding_value=chr2idx['<pad>'])\n",
    "    phone_batch = pad_sequence(phone_batch, batch_first=True, padding_value=ph2idx['<pad>'])\n",
    "    return word_batch, phone_batch\n",
    "\n",
    "class CMUDictDataset(Dataset):\n",
    "    def __init__(self, word_tokens, phone_tokens):\n",
    "        self.word_tokens = word_tokens\n",
    "        self.phone_tokens = phone_tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word_tokens)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.word_tokens[idx]), torch.tensor(self.phone_tokens[idx])\n",
    "\n",
    "dataset = CMUDictDataset(word_tokens, phone_tokens)\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4169286b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(input_size, hidden_size, num_layers=3)\n",
    "decoder = Decoder(output_size, hidden_size, num_layers=3)\n",
    "model = Seq2Seq(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c738df1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.1764: 100%|#########################| 4224/4224 [04:44<00:00, 14.21it/s]"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=ph2idx[pad_token])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 1\n",
    "device = 'cpu'\n",
    "\n",
    "pbar =  tqdm(total=num_epochs * len(dataloader), desc=\"Epochs\", leave=False, position=0, ascii=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for word_batch, phone_batch in dataloader:\n",
    "        word_batch = word_batch.to(device)\n",
    "        phone_batch = phone_batch.to(device)\n",
    "        outputs = model(word_batch, phone_batch)\n",
    "\n",
    "        outputs = outputs.view(-1, outputs.shape[-1])\n",
    "        phone_batch = phone_batch.view(-1)\n",
    "\n",
    "        loss = criterion(outputs, phone_batch)\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "        pbar.update(1)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4d56deba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P', 'R', 'IY0', 'S', 'AY1', 'D', 'AH0', 'N', 'T']\n"
     ]
    }
   ],
   "source": [
    "word = \"president\"\n",
    "\n",
    "word_tokens = torch.tensor([chr2idx[c] for c in word]).reshape(1, -1)\n",
    "phone_tokens = beam_search(model, word_tokens, 10)\n",
    "phones = [idx2ph[tok] for tok in phone_tokens][:-1]\n",
    "print(phones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5712349",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d194c1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, nhead=4, num_layers=2, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.positional_encoding = PositionalEncoding(hidden_size, dropout=dropout)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=hidden_size * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "    def forward(self, src, src_mask=None):\n",
    "        embedded = self.embedding(src)\n",
    "        embedded = self.positional_encoding(embedded)\n",
    "        output =  self.transformer(embedded, src_mask)\n",
    "        return output\n",
    "    \n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size, nhead=4, num_layers=2, dropout=0.1):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.positional_encoding = PositionalEncoding(hidden_size, dropout=dropout)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=hidden_size,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=hidden_size * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerDecoder(decoder_layer, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        embedded = self.embedding(tgt)\n",
    "        embedded = self.positional_encoding(embedded)\n",
    "        output = self.transformer(embedded, memory, tgt_mask, memory_mask)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "    \n",
    "class TransformerSeq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(TransformerSeq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def generate_square_subsequent_mask(self, sz, device):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask.to(device)\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=1.0, device='cpu'):\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt.shape[1], device)\n",
    "        \n",
    "        encoder_output = self.encoder(src)\n",
    "        output = self.decoder(tgt, encoder_output, tgt_mask)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5f1a4db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TransformerEncoder(input_size, hidden_size=128)\n",
    "decoder = TransformerDecoder(output_size, hidden_size=128)\n",
    "model = TransformerSeq2Seq(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bde12b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.5525: 100%|########################9| 4223/4224 [02:09<00:00, 32.22it/s]"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=ph2idx[pad_token])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 1\n",
    "device = 'cpu'\n",
    "\n",
    "pbar =  tqdm(total=num_epochs * len(dataloader), desc=\"Epochs\", leave=False, position=0, ascii=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for word_batch, phone_batch in dataloader:\n",
    "        word_batch = word_batch.to(device)\n",
    "        phone_batch = phone_batch.to(device)\n",
    "        outputs = model(word_batch, phone_batch)\n",
    "\n",
    "        outputs = outputs[:, :-1, :].contiguous()\n",
    "        phone_batch = phone_batch[:, 1:].contiguous()\n",
    "\n",
    "        outputs = outputs.view(-1, outputs.shape[-1])\n",
    "        phone_batch = phone_batch.view(-1)\n",
    "\n",
    "        loss = criterion(outputs, phone_batch)\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "        pbar.update(1)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e864e317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decode(model, src, beam_width=5, max_len=50, device='cpu'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        src = src.to(device)\n",
    "        encoder_output = model.encoder(src)\n",
    "        \n",
    "        beams = [(0, torch.tensor([[ph2idx[sos_token]]], device=device))]\n",
    "        \n",
    "        for _ in range(max_len - 1):\n",
    "            new_beams = []\n",
    "            \n",
    "            for score, sequence in beams:\n",
    "                if sequence[0, -1].item() == ph2idx[eos_token]:\n",
    "                    new_beams.append((score, sequence))\n",
    "                    continue\n",
    "                \n",
    "                tgt_mask = model.generate_square_subsequent_mask(sequence.size(1), device)\n",
    "                \n",
    "                output = model.decoder(sequence, encoder_output, tgt_mask)\n",
    "                \n",
    "                log_probs = F.log_softmax(output[:, -1:], dim=-1)\n",
    "\n",
    "                values, indices = log_probs.squeeze().topk(beam_width)\n",
    "                \n",
    "                for value, idx in zip(values, indices):\n",
    "                    new_score = score + value.item()\n",
    "                    new_sequence = torch.cat([sequence, \n",
    "                                           idx.unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "                    new_beams.append((new_score, new_sequence))\n",
    "            \n",
    "            beams = sorted(new_beams, key=lambda x: x[0], reverse=True)[:beam_width]\n",
    "            \n",
    "            if all(beam[1][0, -1].item() == ph2idx[eos_token] for beam in beams):\n",
    "                break\n",
    "        \n",
    "        return beams[0][1].squeeze(0).cpu().numpy()\n",
    "\n",
    "def predict_pronunciation(model, word, beam_width=5, device='cpu'):\n",
    "    word_tokens = torch.tensor([[chr2idx[c] for c in word.lower() if c in chr2idx]], \n",
    "                             device=device)\n",
    "    \n",
    "    phone_tokens = beam_search_decode(model, word_tokens, beam_width=beam_width, device=device)\n",
    "    phones = [idx2ph[idx] for idx in phone_tokens]\n",
    "    \n",
    "    phones = [p for p in phones if p not in [sos_token, eos_token, pad_token]]\n",
    "    \n",
    "    return phones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9a60014f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['P', 'R', 'IY0', 'Z', 'AY1', 'D', 'AH0', 'N', 'T']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_pronunciation(model, \"president\", beam_width=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb94437",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
